# LMFLow
## evaluation
- NLL
  - negative log-likelihood
  - the low, the better
- QA accuracy
- human
  - PPO
    - 15h with 8xA100
  - RLHF:reinforcement learning from human feedback
## 3xmodel
- language model: LLaMa-SFT-updated
- reward model: LLaMa-RM
- reference model: KL estimation
##
- multi-round
  - 5W QA
  - N~NN hour
  - 3090 24G
- basic model
  - LLaMa 7G, 2048 token
---
---

# overall
## AI(artificial intelligence)
  - ML(machine learning)
    - supervised
    - unsupervised
    - reinforcement
  - DL(deep learning)
    - tool
## AI painting
- 2012: alexnet
- 2014: GANs, generate nonexistent graph
- 2015: neural style transfer
- 2018: DALL-E, text to graph, openAI, low efficient & quality
- 2022: stable diffusion, high efficient & quality, training with low resolution(720x576 better than 4096x3840)
## AI conversation
- 2011: siri
- 2017: transformer, power tool, google
- 2018: google Duplex, robot
- 2018: bert, GPT1,2, insufficient dataset, small model
- 2022: chatGPT=GPT4(2022.08 ~ legal and moral constraints)
- copilot
  - github
  - office365
  - autoGPT
    - online, 202109~Now
    - multi GPT
  - NotionAI
    - GPT3
    - product: summary, inspiration
## compare
-       pretraining   layer   vector_length   model_weight  training_weight
- GPT1  finetune      12      768             0.117B        5GB
- GPT2  zero_shot     48      1600            1.5B          40GB
- GPT3  prompt_tuning 96      12888           175B          45TB
- GPT4  RLHF          96      12888           175B          45TB
## RLHF
- reinforcement learning from human feedback

---
---

# Large model application selection comparison
## dataset
- dify
- fastgpt
- langchatchat
## agent
- flowise
- langflow
- bisheng
## recall ratio improvement


---
---

# from GLM 130B to ChatGLM


---
---


# AIGC
- Artificial Intelligence Generated Content
- https://a16z.com/2023/01/19/who-owns-the-generative-ai-platform/
- infrastructure vendors 
- model providers
- Application companies 

# LLMs
- large language models

# NLP
- natural language processing

# NLU
- natural language understanding